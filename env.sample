ES_URL=
ES_USERNAME=
ES_PASSWORD=
# ES_API_KEY=

# Target index name
#INDEX_NAME=datasets_wikipedia_en

# If INDEX_NAME is not given, target index name will be:
# {INDEX_PREFIX}-{PATH}-{DATASET_NAME}
# where PATH is the DATASET_PATH with / replaced by _
# Default INDEX_PREFIX is "datasets"
#INDEX_PREFIX=datasets

# Dataset information
# You can find the available datasets from:
# https://huggingface.co/datasets/wikimedia/wikipedia/tree/main
DATASET_PATH=wikimedia/wikipedia
DATASET_NAME=20231101.en
# DATASET_NAME=20231101.ja

# Set to true to delete and recreate the index
RENEW_INDEX=false

# Set the number of documents to bulk index per batch
BULK_SIZE=100
# Set the number of documents to ingest
NUM_INGEST=100
# Set the number of threads to use for bulk indexing
BULK_THREADS=8

# Set the random seed for reproducibility
# If not set, it will load the data from the beginning
# RANDOM_SEED=42
